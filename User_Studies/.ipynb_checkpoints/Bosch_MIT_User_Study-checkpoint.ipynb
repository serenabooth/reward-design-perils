{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hungry-Thirsty Domain\n",
    "\n",
    "The goal of the hungry-thirsty domain is to teach an agent to eat as much as possible.\n",
    "<br> There's a catch, though: the agent can only eat when it's not thirsty. <br> Thus, the agent cannot\n",
    "just “hang out” at the food location and keep eating because at\n",
    "some point it will become thirsty and eating will fail.\n",
    "\n",
    "## Rules of the Domain\n",
    "\n",
    "* The agent always exists for 75000 timesteps.\n",
    "* The grid is 6x6. \n",
    "    * (0,0) is the top left\n",
    "    * (0,5) is the top right\n",
    "    * (5,0) is the bottom left\n",
    "    * (5,5) is the bottom right.\n",
    "* Food is located in one randomly-selected corner, while water is located in a different (random) corner.\n",
    "* At each timestep, the agent may take one of the following actions: move (up, down, left, right), eat, or drink, but actions can fail: \n",
    "    * The drink action fails if the agent is not at the water location.\n",
    "    * The eat action fails if the agent is thirsty, or if the agent is not at the food location.\n",
    "    * The move action fails if the agent tries to move through one of the red barriers (depicted below).\n",
    "* If the agent eats, it becomes not-hungry for one timestep.<br>\n",
    "* If the agent drinks, it becomes not-thirsty.<br>\n",
    "* When the agent is not-thirsty, it becomes thirsty again with 10% probability on each timestep.\n",
    "\n",
    "<img src=\"Assets/hungry-thirsty.gif\" width=\"500\"/>\n",
    "\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "<TODO: a description of the underlying alg approach>\n",
    "\n",
    "\n",
    "## Your Objective\n",
    "\n",
    "Your objective is to create the best Hungry-Thirsty agent you can. \n",
    "\n",
    "If your agent is one of the best 5 agents developed in these user studies, you'll receive an additional bonus of $10. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function\n",
    "We haven't specified the reward function; you'll need to create one using the sliders provided below.\n",
    "\n",
    "## Training Algorithm\n",
    "We haven't specified the details of the training algorithm; you'll need to set some parameters using the sliders provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup_reward_fn.ipynb\n",
    "%run setup_learning_alg.ipynb\n",
    "%run training_and_model_eval.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soliciting Rewards\n",
    "Your task: Assign a value r(s') to each state. \n",
    "\n",
    "The total reward is the **sum** of the predicate rewards and position rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96eae8f501424e45b1679ffb5b934b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Box(children=(HBox(children=(Label(value='HUNGRY_AND_THIRSTY'), FloatSlider(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solicit_rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for predicate values: \n",
      "    HUNGRY_AND_THIRSTY: 0.0\n",
      "    HUNGRY_AND_NOT_THIRSTY: 0.0\n",
      "    NOT_HUNGRY_AND_THIRSTY: 0.0\n",
      "    NOT_HUNGRY_AND_NOT_THIRSTY: 0.0\n",
      "Reward for position values: \n",
      "    (0, 0): 0.0\n",
      "    (0, 1): 0.0\n",
      "    (0, 2): 0.0\n",
      "    (0, 3): 0.0\n",
      "    (0, 4): 0.0\n",
      "    (0, 5): 0.0\n",
      "    (1, 0): 0.0\n",
      "    (1, 1): 0.0\n",
      "    (1, 2): 0.0\n",
      "    (1, 3): 0.0\n",
      "    (1, 4): 0.0\n",
      "    (1, 5): 0.0\n",
      "    (2, 0): 0.0\n",
      "    (2, 1): 0.0\n",
      "    (2, 2): 0.0\n",
      "    (2, 3): 0.0\n",
      "    (2, 4): 0.0\n",
      "    (2, 5): 0.0\n",
      "    (3, 0): 0.0\n",
      "    (3, 1): 0.0\n",
      "    (3, 2): 0.0\n",
      "    (3, 3): 0.0\n",
      "    (3, 4): 0.0\n",
      "    (3, 5): 0.0\n",
      "    (4, 0): 0.0\n",
      "    (4, 1): 0.0\n",
      "    (4, 2): 0.0\n",
      "    (4, 3): 0.0\n",
      "    (4, 4): 0.0\n",
      "    (4, 5): 0.0\n",
      "    (5, 0): 0.0\n",
      "    (5, 1): 0.0\n",
      "    (5, 2): 0.0\n",
      "    (5, 3): 0.0\n",
      "    (5, 4): 0.0\n",
      "    (5, 5): 0.0\n"
     ]
    }
   ],
   "source": [
    "print_reward_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Your task: Choose hyperparameters for the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1e81db7413406ba5c917a95c06c2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Dropdown(description='gamma', options=(0, 0.5, 0.8, 0.9, 0.99), style=DescriptionStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_alg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward function params: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_learning_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_422904/3954892145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do not edit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Reward function params: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_reward_fn_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning algorithm params: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_learning_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#train_agent(reward_fn_params=get_reward_fn_params(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_learning_params' is not defined"
     ]
    }
   ],
   "source": [
    "# do not edit \n",
    "print (\"Reward function params: {}\".format(get_reward_fn_params()))\n",
    "print (\"Learning algorithm params: {}\".format(get_learning_params()))\n",
    "\n",
    "#train_agent(reward_fn_params=get_reward_fn_params(), \n",
    "#            num_environments=hyperparam_widgets['num_trials'].value, \n",
    "#            alpha=hyperparam_widgets['alpha'].value, \n",
    "#            epsilon=hyperparam_widgets['epsilon'].value,\n",
    "#            gamma=hyperparam_widgets['gamma'].value, \n",
    "#            env_timesteps=hyperparam_widgets['episode_length'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
