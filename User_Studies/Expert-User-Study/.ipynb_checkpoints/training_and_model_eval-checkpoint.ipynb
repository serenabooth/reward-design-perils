{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "gym.logger.set_level(40)\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "sys.path.insert(0,'../../RL_algorithms')\n",
    "import A2C\n",
    "import DDQN\n",
    "import PPO\n",
    "import pprint\n",
    "import ipywidgets as widgets\n",
    "import Utils\n",
    "import csv\n",
    "from IPython import display\n",
    "\n",
    "import gym_hungry_thirsty\n",
    "env_name = \"hungry-thirsty-v0\"\n",
    "env = gym.make(env_name, size=(4,4))\n",
    "\n",
    "your_attempts = {}\n",
    "your_attempts_idx = 0\n",
    "\n",
    "perf_tracking = {}\n",
    "\n",
    "show_agent_window = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c(hyper_params, reward_fn):\n",
    "    \"\"\"\n",
    "    Construct an A2C agent.\n",
    "    \"\"\"\n",
    "    return A2C.create_a2c_agent(env=env, hyper_params=hyper_params, user_reward_fn=reward_fn, plot_state_visit_distribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(hyper_params, reward_fn):\n",
    "    \"\"\"\n",
    "    Construct a PPO agent.\n",
    "    \"\"\"\n",
    "    return PPO.create_ppo_agent(env=env, hyper_params=hyper_params, user_reward_fn=reward_fn, plot_state_visit_distribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn(hyper_params, reward_fn):\n",
    "    \"\"\"\n",
    "    Construct a DQN agent.\n",
    "    \"\"\"\n",
    "    return DDQN.create_ddqn_agent(env=env, hyper_params=hyper_params, user_reward_fn=reward_fn, plot_state_visit_distribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = None\n",
    "epsilon = None\n",
    "\n",
    "def train_agent(alg_and_reward_params, hyper_params, study_id):\n",
    "    \"\"\"\n",
    "    Train the agent \n",
    "    \"\"\"\n",
    "    global agent, epsilon, your_attempts, your_attempts_idx, perf_tracking\n",
    "    \n",
    "    \n",
    "    # do some error checking to make sure the hyperparameters are set before proceeding \n",
    "    if hyper_params is None:\n",
    "        print (\"No hyper-parameters are set. Please set these before training.\")\n",
    "        return \n",
    "    if None in alg_and_reward_params.values() or None in hyper_params.values():\n",
    "        not_set = []\n",
    "        for param in alg_and_reward_params:\n",
    "            if alg_and_reward_params[param] is None:\n",
    "                not_set.append(param)\n",
    "        for param in hyper_params:\n",
    "            if hyper_params[param] is None:\n",
    "                not_set.append(param)\n",
    "        print (\"The following hyperparameters are not set: {}.\\nPlease make a selection and try again.\".format(not_set))\n",
    "        return \n",
    "        \n",
    "    def compute_reward(state, action, new_state):\n",
    "        \"\"\"\n",
    "        Using the user-provided weights, compute the reward function\n",
    "        \"\"\"\n",
    "        if state[\"hungry\"] and state[\"thirsty\"]:\n",
    "            return hyper_params[\"reward_scaling_factor\"] * alg_and_reward_params[\"Reward for state: hungry AND thirsty\"]\n",
    "        elif state[\"hungry\"] and not state[\"thirsty\"]:\n",
    "            return hyper_params[\"reward_scaling_factor\"] * alg_and_reward_params[\"Reward for state: hungry AND not thirsty\"]\n",
    "        elif not state[\"hungry\"] and state[\"thirsty\"]:\n",
    "            return hyper_params[\"reward_scaling_factor\"] * alg_and_reward_params[\"Reward for state: not hungry AND thirsty\"]\n",
    "        elif not state[\"hungry\"] and not state[\"thirsty\"]:\n",
    "            return hyper_params[\"reward_scaling_factor\"] * alg_and_reward_params[\"Reward for state: not hungry AND not thirsty\"]\n",
    "        raise Exception(\"Reward computation failed\")\n",
    "    \n",
    "    # save a copy of the hyperparams set by the user \n",
    "    user_specified_hyper_params = hyper_params.copy()\n",
    "    # and add some other hyperparams which may be necessary \n",
    "    hyper_params[\"max_steps\"] = 200 \n",
    "    hyper_params[\"sync_frequency\"] = 5\n",
    "    hyper_params[\"plot_update_freq\"] = 100\n",
    "    hyper_params[\"neural_net_hidden_size\"] = 144\n",
    "    hyper_params[\"neural_net_extra_layers\"] = 0\n",
    "    hyper_params[\"exp_replay_size\"] = 5000\n",
    "    hyper_params[\"K_epochs\"] = 80 \n",
    "        \n",
    "    alg = alg_and_reward_params[\"Algorithm Choice\"]\n",
    "    \n",
    "    trial_idx = your_attempts_idx\n",
    "    \n",
    "    # save params and performance to dicts ('your_attempts' and 'perf_tracking')\n",
    "    env.reset(new_water_food_loc=True)\n",
    "\n",
    "    your_attempts[trial_idx] = {\"alg choice\": alg, \n",
    "                                \"reward\": {\"hungry and thirsty\": alg_and_reward_params[\"Reward for state: hungry AND thirsty\"],\n",
    "                                           \"hungry and not thirsty\": alg_and_reward_params[\"Reward for state: hungry AND not thirsty\"],\n",
    "                                           \"not hungry and thirsty\": alg_and_reward_params[\"Reward for state: not hungry AND thirsty\"],\n",
    "                                           \"not hungry and not thirsty\": alg_and_reward_params[\"Reward for state: not hungry AND not thirsty\"]}, \n",
    "                                \"hyper_params\": user_specified_hyper_params,\n",
    "                                \"food_loc\": env.food_loc,\n",
    "                                \"water_loc\": env.water_loc,\n",
    "                               }\n",
    "        \n",
    "    perf_tracking[trial_idx] = {\"agent\": None,\n",
    "                                \"rewards\": [], \n",
    "                                \"fitness\": []}\n",
    "    \n",
    "    fields=['trial', 'alg', 'reward_fn', 'hyper_params']\n",
    "    csv_filename = \"user_tests/\" + study_id +\".csv\"\n",
    "    csv_file_exists = os.path.isfile(csv_filename)\n",
    "    \n",
    "    with open(csv_filename, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not csv_file_exists:\n",
    "            writer.writerow(fields)\n",
    "\n",
    "        writer.writerow([trial_idx, \n",
    "                         your_attempts[trial_idx]['alg choice'],\n",
    "                         your_attempts[trial_idx]['reward'],\n",
    "                         your_attempts[trial_idx]['hyper_params']])\n",
    "    \n",
    "    your_attempts_idx += 1\n",
    "\n",
    "    \n",
    "    if alg == \"A2C\":\n",
    "        agent, rewards, fitness = a2c(hyper_params, reward_fn=compute_reward)\n",
    "    elif alg == \"PPO\":\n",
    "        agent, rewards, fitness = ppo(hyper_params, reward_fn=compute_reward)\n",
    "    elif alg == \"DDQN\":\n",
    "        agent, epsilon, rewards, fitness = ddqn(hyper_params, reward_fn=compute_reward)\n",
    "    else:\n",
    "        print (\"ERROR, {} not implemented yet\".format(alg))\n",
    "        \n",
    "    perf_tracking[trial_idx] = {\"agent\": agent,\n",
    "                                \"rewards\": rewards, \n",
    "                                \"fitness\": fitness}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_run_and_show_agent():\n",
    "    global your_attempts, show_agent_window \n",
    "    \n",
    "    if not your_attempts:\n",
    "        print (\"You have not yet finished training an agent.\")\n",
    "        return \n",
    "    \n",
    "    if show_agent_window is not None: \n",
    "        show_agent_window.clear_output()\n",
    "    else:\n",
    "        show_agent_window = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "    options = []\n",
    "    for option in list(your_attempts.keys()):\n",
    "        options.insert(0, (\"Trial: {}; Alg: {}\".format(option, your_attempts[option][\"alg choice\"]), option))\n",
    "    \n",
    "        \n",
    "    select_run = widgets.Dropdown(\n",
    "        options=options,\n",
    "        description='Select trial:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    button = widgets.Button(description=\"See Agent\")\n",
    "    \n",
    "    select_and_enter = widgets.VBox(children=[widgets.Box(children=[select_run, button]),show_agent_window])\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        show_agent_window.clear_output()\n",
    "        with show_agent_window:\n",
    "            alg = your_attempts[select_run.value]['alg choice']\n",
    "            agent = perf_tracking[select_run.value][\"agent\"]\n",
    "            show_agent(agent=agent, alg=alg, trial_id=select_run.value)\n",
    "\n",
    "        \n",
    "    button.on_click(on_button_clicked)\n",
    "    return select_and_enter\n",
    "\n",
    "def show_agent(agent, alg, trial_id):\n",
    "    \"\"\"\n",
    "    Visualize the agent's performance \n",
    "    \"\"\"\n",
    "    global epsilon\n",
    "    \n",
    "    if alg == None or alg == \"None\":\n",
    "        print (\"You have not yet selected an algorithm (A2C, PPO, or DDQN).\")\n",
    "        return \n",
    "    elif agent == None:\n",
    "        print (\"You have not yet finished training an agent.\")\n",
    "        return \n",
    "\n",
    "    view_training_runs(trial_id=trial_id)\n",
    "    food_loc = your_attempts[trial_id][\"food_loc\"]\n",
    "    water_loc = your_attempts[trial_id][\"water_loc\"]\n",
    "\n",
    "    env.reset(food_loc=food_loc, water_loc=water_loc)\n",
    "        \n",
    "    wIm = widgets.Image()\n",
    "    display.display(wIm)\n",
    "\n",
    "    if alg == \"A2C\":\n",
    "        A2C.run_episode(env=env, \n",
    "                        actor=agent, \n",
    "                        render=True, \n",
    "                        jupyter=True,\n",
    "                        canvas=wIm)\n",
    "    elif alg == \"PPO\":\n",
    "        PPO.run_episode(env=env, \n",
    "                        ppo_agent=agent, \n",
    "                        render=True, \n",
    "                        jupyter=True,\n",
    "                        canvas=wIm)\n",
    "    elif alg == \"DDQN\":\n",
    "        DDQN.run_episode(env=env, \n",
    "                        agent=agent, \n",
    "                        epsilon=epsilon, \n",
    "                        render=True, \n",
    "                        jupyter=True,\n",
    "                        canvas=wIm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review your selections for each of your training runs\n",
    "def view_training_runs(trial_id=None):\n",
    "    \"\"\"\n",
    "    Pretty print 'your_attempts' dict\n",
    "    \"\"\"\n",
    "    global your_attempts\n",
    "\n",
    "    if not your_attempts:\n",
    "        print (\"You have not yet trained an agent.\")\n",
    "        return \n",
    "    \n",
    "    if trial_id is None:\n",
    "        trial_set = your_attempts.keys()\n",
    "    else:\n",
    "        trial_set = [trial_id]\n",
    "    \n",
    "    for i in trial_set:\n",
    "        print (\"Trial {}-----------------------------------------\".format(i))\n",
    "        print (\"     {:10s}: {}\".format('Algorithm', your_attempts[i]['alg choice']))\n",
    "\n",
    "        print (\"     {:10s}\".format('Reward Function:'))\n",
    "        for j in your_attempts[i]['reward'].keys():\n",
    "            print(\"          {:10s}: {:1.2f}\".format(\"r(\" + j + \")\",\n",
    "                                                     your_attempts[i]['reward'][j]))\n",
    "\n",
    "        print (\"     {:10s}\".format('Hyper-parameters:'))\n",
    "        for j in your_attempts[i]['hyper_params'].keys():\n",
    "            print(\"          {:10s}: {:1.5f}\".format(j,\n",
    "                                                     your_attempts[i]['hyper_params'][j]))\n",
    "\n",
    "\n",
    "        print (\"------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_past_run():\n",
    "    global your_attempts\n",
    "\n",
    "    if not your_attempts:\n",
    "        print (\"You have not yet trained an agent.\")\n",
    "        return \n",
    "\n",
    "    out = widgets.Output(layout={'border': '1px solid black'})\n",
    "    \n",
    "    options = []\n",
    "    for option in list(your_attempts.keys()):\n",
    "        options.insert(0, (\"Trial: {}; Alg: {}\".format(option, your_attempts[option][\"alg choice\"]), option))\n",
    "        \n",
    "    select_run = widgets.Dropdown(\n",
    "        options=options,\n",
    "        description='Select trial:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    button = widgets.Button(description=\"See Trial Metrics\")\n",
    "    \n",
    "    select_and_enter = widgets.VBox(children=[widgets.Box(children=[select_run, button]),out])\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        out.clear_output()\n",
    "        with out: \n",
    "            plotting = {\n",
    "                0: (\"Not Hungry Count Per Episode\\n\" +\n",
    "                    r\"$\\Sigma_{(s, a, s') \\in \\tau} \\mathbb{1}(s\\mathrm{[is\\_hungry]=False)}$\",\n",
    "                    \"fitness\", (\"Episode\", \"Not Hungry Count\"), True),\n",
    "                1: (\"Undiscounted Return\\n\" +\n",
    "                    r\"Summed Reward Per Episode: $\\Sigma_{(s, a, s') \\in \\tau} r'(s)$\",\n",
    "                    \"rewards\", (\"Episode\", \"Return\"), False),\n",
    "            }\n",
    "\n",
    "\n",
    "            fig = Utils.InteractiveLearningCurvePlot(num_axes=len(plotting.items()))\n",
    "\n",
    "            for idx in plotting.keys():\n",
    "                title, list_name, labels, draw_scaling_lines = plotting[idx]\n",
    "                plotting_data = perf_tracking[select_run.value][list_name]\n",
    "                if len(plotting_data) > 1:\n",
    "                    fig.update_subplot(axis_id=idx,\n",
    "                                       title=title,\n",
    "                                       learning_performance=plotting_data,\n",
    "                                       labels=labels, \n",
    "                                       draw_scaling_lines=draw_scaling_lines)\n",
    "\n",
    "            view_training_runs(trial_id=select_run.value)\n",
    "        \n",
    "    button.on_click(on_button_clicked)\n",
    "    return select_and_enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_agent():\n",
    "    global your_attempts\n",
    "\n",
    "    if not your_attempts:\n",
    "        print (\"You have not yet trained an agent.\")\n",
    "        return \n",
    "    \n",
    "    options = []\n",
    "    for option in list(your_attempts.keys()):\n",
    "        options.append((\"Trial: {}; Alg: {}\".format(option, your_attempts[option][\"alg choice\"]), option))\n",
    "        \n",
    "    select_run = widgets.Dropdown(\n",
    "        options=options,\n",
    "        description='Select trial:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    button = widgets.Button(description=\"Submit\")    \n",
    "    \n",
    "    final_selection = widgets.HBox(children=[select_run, button])\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        selected_trial = select_run.value\n",
    "        f = open(\"user_tests/\" + study_id +\".txt\", \"w\")\n",
    "        f.write(\"Selected agent: {}\\n\".format(selected_trial))\n",
    "        f.write(\"ALG: {}\\n\".format(your_attempts[selected_trial]['alg choice']))\n",
    "        f.write(\"REWARD: {}\\n\".format(your_attempts[selected_trial]['reward']))\n",
    "        f.write(\"HYPER_PARAMS: {}\".format(your_attempts[selected_trial]['hyper_params']))\n",
    "        print (\"You selected trial: {}\".format(selected_trial))\n",
    "        view_training_runs(trial_id=selected_trial)\n",
    "        \n",
    "    button.on_click(on_button_clicked)\n",
    "    \n",
    "    return final_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper button.btn.btn-default,\n",
       ".output_wrapper .ui-dialog-titlebar {\n",
       "  display: none;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper button.btn.btn-default,\n",
    ".output_wrapper .ui-dialog-titlebar {\n",
    "  display: none;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
