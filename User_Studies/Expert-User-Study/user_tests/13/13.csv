trial,alg,reward_fn,hyper_params
0,A2C,"{'hungry and thirsty': -1.0, 'hungry and not thirsty': 0.0, 'not hungry and thirsty': -0.5, 'not hungry and not thirsty': 1.0}","{'gamma': 0.99, 'num_episodes': 1000, 'lr': 0.0001, 'entropy_coeff': 0.01, 'n_step_update': 32, 'reward_scaling_factor': 1}"
1,A2C,"{'hungry and thirsty': -1.0, 'hungry and not thirsty': -0.5, 'not hungry and thirsty': -0.5, 'not hungry and not thirsty': 1.0}","{'gamma': 0.99, 'num_episodes': 1000, 'lr': 1e-05, 'entropy_coeff': 0.05, 'n_step_update': 32, 'reward_scaling_factor': 1}"
2,A2C,"{'hungry and thirsty': -0.5, 'hungry and not thirsty': 0.0, 'not hungry and thirsty': 0.0, 'not hungry and not thirsty': 1.0}","{'gamma': 0.99, 'num_episodes': 1000, 'lr': 1e-05, 'entropy_coeff': 0.05, 'n_step_update': 32, 'reward_scaling_factor': 1}"
3,PPO,"{'hungry and thirsty': -0.5, 'hungry and not thirsty': 0.0, 'not hungry and thirsty': 0.0, 'not hungry and not thirsty': 1.0}","{'gamma': 0.99, 'num_episodes': 1000, 'lr': 0.001, 'update_steps': 64, 'eps_clip': 0.1, 'entropy_coeff': 0.1, 'reward_scaling_factor': 1}"
4,PPO,"{'hungry and thirsty': -0.5, 'hungry and not thirsty': 0.0, 'not hungry and thirsty': 0.5, 'not hungry and not thirsty': 1.0}","{'gamma': 0.99, 'num_episodes': 1000, 'lr': 0.001, 'update_steps': 64, 'eps_clip': 0.1, 'entropy_coeff': 0.1, 'reward_scaling_factor': 1}"
5,PPO,"{'hungry and thirsty': -0.1, 'hungry and not thirsty': 0.0, 'not hungry and thirsty': 0.0, 'not hungry and not thirsty': 1.0}","{'gamma': 0.99, 'num_episodes': 2500, 'lr': 0.001, 'update_steps': 64, 'eps_clip': 0, 'entropy_coeff': 0.01, 'reward_scaling_factor': 1}"
6,PPO,"{'hungry and thirsty': -0.1, 'hungry and not thirsty': 0.0, 'not hungry and thirsty': 0.0, 'not hungry and not thirsty': 1.0}","{'gamma': 0.9, 'num_episodes': 2500, 'lr': 0.001, 'update_steps': 64, 'eps_clip': 0, 'entropy_coeff': 0.01, 'reward_scaling_factor': 1}"
7,PPO,"{'hungry and thirsty': -0.4, 'hungry and not thirsty': -0.5, 'not hungry and thirsty': 0.0, 'not hungry and not thirsty': 1.0}","{'gamma': 0.9, 'num_episodes': 2500, 'lr': 0.001, 'update_steps': 64, 'eps_clip': 0.1, 'entropy_coeff': 0.01, 'reward_scaling_factor': 1}"
